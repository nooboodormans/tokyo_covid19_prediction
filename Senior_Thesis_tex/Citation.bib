@misc{Kanagawa_prediction,
	title = {Simulation of the number of severe cases using a new COVID-19 prediction model},
	author = {Kanagawa Prefecture Official, Kanagawa University of Human Services},
	url = {https://www.pref.kanagawa.jp/docs/ga4/covid19/simulation.html},
	language = {ja},
	year = {2022},
	urldate = {2022-01-05},
}

@incollection{brooks_univariate_2008,
	address = {Cambridge},
	edition = {2},
	title = {Univariate time series modelling and forecasting},
	isbn = {9781107795204},
	url = {https://www.cambridge.org/core/books/introductory-econometrics-for-finance/univariate-time-series-modelling-and-forecasting/F20C6744BB368689ED4B6C5333BA773F},
	abstract = {Learning OutcomesIn this chapter, you will learn how toExplain the defining characteristics of various types of stochastic processesIdentify the appropriate time series model for a given data seriesProduce forecasts for ARMA and exponential smoothing modelsEvaluate the accuracy of predictions using various metricsEstimate time series models and produce forecasts from them in EViewsIntroductionUnivariate time series models are a class of specifications where one attempts to model and to predict financial variables using only information contained in their own past values and possibly current and past values of an error term. This practice can be contrasted with structural models, which are multivariate in nature, and attempt to explain changes in a variable by reference to the movements in the current or past values of other (explanatory) variables. Time series models are usually a-theoretical, implying that their construction and use is not based upon any underlying theoretical model of the behaviour of a variable. Instead, time series models are an attempt to capture empirically relevant features of the observed data that may have arisen from a variety of different (but unspecified) structural models. An important class of time series models is the family of AutoRegressive Integrated Moving Average (ARIMA) models, usually associated with Box and Jenkins (1976). Time series models may be useful when a structural model is inappropriate. For example, suppose that there is some variable yt whose movements a researcher wishes to explain.},
	booktitle = {Introductory {Econometrics} for {Finance}},
	publisher = {Cambridge University Press},
	author = {Brooks, Chris},
	year = {2008},
	doi = {10.1017/CBO9780511841644.006},
	pages = {206--264},
}

@incollection{noauthor_introductory_2012,
	title = {Introductory {Econometrics} for {Finance}},
	publisher = {Cambridge University Press},
	month = jun,
	year = {2012},
}

@misc{tokyo_foot_traffic,
    title={Response to the novel coronavirus infection (COVID-19) Office of Infectious Disease Control and Prevention}, 
    url={https://corona.go.jp/dashboard/}, 
    urldate = {2022-01-05},
    author = {Cabinet Secretariat of Japan},
    year = {2022},
}

@misc{tokyo_pcr,
	title = {Positive rate and number of people tested for novel coronavirus infection in Tokyo - Tokyo Metropolitan Government Open Data Catalog Site},
	url = {https://catalog.data.metro.tokyo.lg.jp/dataset/t000010d0000000088/resource/94019800-4577-44a2-9365-cf5a2fd28040},
	author = {Tokyo Metropolitan Government},
	urldate = {2022-01-05},
	year = {2022},
}

@misc{tokyo_COVID,
	title = {Situation regarding coronavirus infected patients in Tokyo - Tokyo Metropolitan Government Open Data Catalog Site},
	url = {https://catalog.data.metro.tokyo.lg.jp/dataset/t000010d0000000089/resource/54996023-7255-45c5-b5b0-60458d874715},
	author = {Tokyo Metropolitan Government},
	language = {ja},
	urldate = {2022-01-05},
	year = {2022},
}

@article{granger_spurious_1974,
	title = {Spurious regressions in econometrics},
	volume = {2},
	doi = {10.1016/0304-4076(74)90034-7},
	number = {Journal of Econometrics},
	journal = {North-Holland Publishing Company},
	author = {Granger, C.W.J and Newbold, P},
	year = {1974},
	pages = {111--120},
}

@online{oguntayo_preprocessing_2020,
	title = {Preprocessing {Time} {Series} {Data} for {Supervised} {Learning}},
	url = {https://towardsdatascience.com/preprocessing-time-series-data-for-supervised-learning-2e27493f44ae},
	abstract = {Using traditional ML algorithms with Time Series},
	language = {en},
	urldate = {2022-01-04},
	journal = {Medium},
	author = {Oguntayo, Sijuade},
	month = dec,
	year = {2020},
}

@online{noauthor_developing_2021,
	title = {Developing {Vector} {AutoRegressive} {Model} in {Python}!},
	url = {https://www.analyticsvidhya.com/blog/2021/08/vector-autoregressive-model-in-python/},
	abstract = {Vector AutoRegressive (VAR) is a multivariate forecasting algorithm that is used when two or more time series influence each other.},
	language = {en},
	urldate = {2022-01-02},
	journal = {Analytics Vidhya},
	month = aug,
	year = {2021},
}

@online{noauthor_multivariate_2018,
	title = {Multivariate {Time} {Series} {\textbar} {Vector} {Auto} {Regression} ({VAR})},
	url = {https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/},
	abstract = {Vector Auto Regression method for forecasting multivariate time series uses vectors to represent the relationship between variables and past values.},
	language = {en},
	urldate = {2022-01-02},
	journal = {Analytics Vidhya},
	month = sep,
	year = {2018},
}

@online{noauthor_time_nodate,
	title = {Time {Series} {Prediction} with {LSTM} {Recurrent} {Neural} {Networks} in {Python} with {Keras}},
	url = {https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/},
	urldate = {2022-01-02},
}

@inproceedings{Comparison_ARIMA_LSTM,                   
    author={Siami-Namini, Neda Tavakoli, Akbar Siami Namin}, 
    booktitle={2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)},   
    title={A Comparison of ARIMA and LSTM in Forecasting Time Series},   
    year={2018},  
    volume={},  
    number={},  
    pages={1394-1401},  doi={10.1109/ICMLA.2018.00227}
}

@article{Time_Series_forecasting_ARIMA,
    title = {Time series forecasting using a hybrid ARIMA and neural network model},
    journal = {Neurocomputing},
    volume = {50},
    pages = {159-175},
    year = {2003},
    issn = {0925-2312},
    doi = {https://doi.org/10.1016/S0925-2312(01)00702-0},
    url = {https://www.sciencedirect.com/science/article/pii/S0925231201007020},
    author = {G.Peter Zhang},
    keywords = {ARIMA, Box–Jenkins methodology, Artificial neural networks, Time series forecasting},
    abstract = {Autoregressive integrated moving average (ARIMA) is one of the popular linear models in time series forecasting during the past three decades. Recent research activities in forecasting with artificial neural networks (ANNs) suggest that ANNs can be a promising alternative to the traditional linear methods. ARIMA models and ANNs are often compared with mixed conclusions in terms of the superiority in forecasting performance. In this paper, a hybrid methodology that combines both ARIMA and ANN models is proposed to take advantage of the unique strength of ARIMA and ANN models in linear and nonlinear modeling. Experimental results with real data sets indicate that the combined model can be an effective way to improve forecasting accuracy achieved by either of the models used separately.}
}

@article{LSTM,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf}
}

@article{Comparison_GRU_LSTM,  
    author={Yang, Shudong and Yu, Xueying and Zhou, Ying},  
    booktitle={2020 International Workshop on Electronic Communication and Artificial Intelligence (IWECAI)},   
    title={LSTM and GRU Neural Network Performance Comparison Study: Taking Yelp Review Dataset as an Example},   
    year={2020},  
    volume={},  
    number={},  
    pages={98-101},  
    doi={10.1109/IWECAI50956.2020.00027}
}

@misc{GRU,
      title={On the Properties of Neural Machine Translation: Encoder-Decoder Approaches}, 
      author={Kyunghyun Cho and Bart van Merrienboer and Dzmitry Bahdanau and Yoshua Bengio},
      year={2014},
      eprint={1409.1259},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{GRU_performance,
    title = {Deep Learning with Gated Recurrent Unit Networks for Financial Sequence Predictions},
    journal = {Procedia Computer Science},
    volume = {131},
    pages = {895-903},
    year = {2018},
    note = {Recent Advancement in Information and Communication Technology:},
    issn = {1877-0509},
    doi = {https://doi.org/10.1016/j.procs.2018.04.298},
    url = {https://www.sciencedirect.com/science/article/pii/S1877050918306781},
    author = {Guizhu Shen and Qingping Tan and Haoyu Zhang and Ping Zeng and Jianjun Xu},
    keywords = {Deep learning, GRU, SVM, financial time series, stock index},
    abstract = {Gated recurrent unit (GRU) networks perform well in sequence learning tasks and overcome the problems of vanishing and explosion of gradients in traditional recurrent neural networks (RNNs) when learning long-term dependencies. Although they apply essentially to financial time series predictions, they are seldom used in the field. To fill this void, we propose GRU networks and its improved version for predicting trading signals for stock indexes of the Hang Seng Indexes (HSI), the Deutscher Aktienindex (DAX) and the S&P 500 Index from 1991 to 2017, and compare the GRU-based models with the traditional deep net and the benchmark classifier support vector machine (SVM). Experimental results show that the two GRU models proposed in this paper both obtain higher prediction accuracy on these data sets, and the improved version can effectively improve the learning ability of the model.}
}

@article{ADF,
    author = {Dickey, D. and Fuller, Wayne},
    year = {1979},
    month = {06},
    pages = {},
    title = {Distribution of the Estimators for Autoregressive Time Series With a Unit Root},
    volume = {74},
    journal = {JASA. Journal of the American Statistical Association},
    doi = {10.2307/2286348}
}

@book{ADF_test_statistic,
    publisher = {Prentice Hall},
    booktitle = {Econometric analysis},
    isbn = {0131108492},
    year = {2003},
    title = {Econometric analysis / William H. Greene.},
    edition = {5th ed.},
    language = {eng},
    address = {Upper Saddle River, N.J.},
    author = {Greene, William H.},
    keywords = {Econometrics},
    lccn = {2002029308},
    pages = "608-662",
    chapter = 20,
}

@misc{statsmodels_ADF,
	title = {statsmodels.tsa.stattools.adfuller — statsmodels},
    author={Seabold, Skipper and Perktold, Josef},
	year = {2010},
	url = {https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.adfuller.html},
	urldate = {2022-01-10},
}

@misc{statsmodels_VIF,
	title = {statsmodels.stats.outliers\_influence — statsmodels},
    author={Seabold, Skipper and Perktold, Josef},
	year = {2010},
	url = {https://www.statsmodels.org/dev/_modules/statsmodels/stats/outliers_influence.html#variance_inflation_factor},
	urldate = {2022-01-11},
}

@book{multicollinearity_problem,
    publisher = {Springer US},
    author = {Michael Patrick Allen},
    booktitle = {Understanding {Regression} {Analysis}},
	isbn = {978-0-585-25657-3},
	year = {1997},
	title = {The problem of multicollinearity},
	pages = "176--180",
}

@article{multicollinearity_VIF,
    author = {Alin, Aylin},
    title = {Multicollinearity},
    journal = {WIREs Computational Statistics},
    volume = {2},
    number = {3},
    pages = {370-374},
    keywords = {collinearity, correlation, ill-conditioned data, linear regression, multicollinearity},
    doi = {https://doi.org/10.1002/wics.84},
    url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.84},
    eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.84},
    abstract = {Abstract Multicollinearity refers to the linear relation among two or more variables. It is a data problem which may cause serious difficulty with the reliability of the estimates of the model parameters. In this article, multicollinearity among the explanatory variables in the multiple linear regression model is considered. Its effects on the linear regression model and some multicollinearity diagnostics for this model are presented. Copyright © 2010 John Wiley \& Sons, Inc. This article is categorized under: Statistical Models > Linear Models Statistical Models > Multivariate Models},
    year = {2010}
}
@incollection{VARMA,
    title = {Chapter 6 Forecasting with VARMA Models},
    booktitle = {Handbook of Economic Forecasting},
    editor = {G. Elliott and C.W.J. Granger and A. Timmermann},
    series = {Handbook of Economic Forecasting},
    publisher = {Elsevier},
    volume = {1},
    pages = {287-325},
    year = {2006},
    issn = {1574-0706},
    doi = {https://doi.org/10.1016/S1574-0706(05)01006-2},
    url = {https://www.sciencedirect.com/science/article/pii/S1574070605010062},
    author = {Helmut Lütkepohl},
    keywords = {echelon form, Kronecker indices, model selection, vector autoregressive process, vector error correction model, cointegration},
    abstract = {Vector autoregressive moving-average (VARMA) processes are suitable models for producing linear forecasts of sets of time series variables. They provide parsimonious representations of linear data generation processes. The setup for these processes in the presence of stationary and cointegrated variables is considered. Moreover, unique or identified parameterizations based on the echelon form are presented. Model specification, estimation, model checking and forecasting are discussed. Special attention is paid to forecasting issues related to contemporaneously and temporally aggregated VARMA processes. Predictors for aggregated variables based alternatively on past information in the aggregated variables or on disaggregated information are compared.}
}
@article{AIC,
    author={Akaike, H.},
    journal={IEEE Transactions on Automatic Control}, 
    title={A new look at the statistical model identification}, 
    year={1974},
    volume={19},
    number={6},
    pages={716-723},
    abstract={The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.},
    keywords={},
    doi={10.1109/TAC.1974.1100705},
    ISSN={1558-2523},
    month={December},
}
@article{BIC,
    ISSN = {00905364},
    URL = {http://www.jstor.org/stable/2958889},
    abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
    author = {Gideon Schwarz},
    journal = {The Annals of Statistics},
    number = {2},
    pages = {461--464},
    publisher = {Institute of Mathematical Statistics},
    title = {Estimating the Dimension of a Model},
    volume = {6},
    year = {1978}
}
@incollection{AIC_BIC,
    title = {Chapter 4 - Bayesian model selection for high-dimensional data},
    editor = {Arni S.R. {Srinivasa Rao} and C.R. Rao},
    series = {Handbook of Statistics},
    publisher = {Elsevier},
    volume = {43},
    pages = {207-248},
    year = {2020},
    booktitle = {Principles and Methods for Data Science},
    issn = {0169-7161},
    doi = {https://doi.org/10.1016/bs.host.2019.08.001},
    url = {https://www.sciencedirect.com/science/article/pii/S0169716119300380},
    author = {Naveen Naidu Narisetty},
    keywords = {Bayesian variable selection, High-dimensional data, Model comparison, Bayesian computation},
    abstract = {High-dimensional data, where the number of features or covariates can even be larger than the number of independent samples, are ubiquitous and are encountered on a regular basis by statistical scientists both in academia and in industry. A majority of the classical research in statistics dealt with the settings where there is a small number of covariates. Due to the modern advancements in data storage and computational power, the high-dimensional data revolution has significantly occupied mainstream statistical research. In gene expression datasets, for instance, it is not uncommon to encounter datasets with observations on at most a few hundred independent samples (subjects) and with information on tens or hundreds of thousands of genes per each sample. An important and common question that arises quickly is—“which of the available covariates are relevant to the outcome of interest?” This concerns the problem of variable selection (and more generally model selection) in statistics and data science. This chapter will provide an overview of some of the most well-known model selection methods along with some of the more recent methods. While frequentist methods will be discussed, Bayesian approaches will be given a more elaborate treatment. The frequentist framework for model selection is primarily based on penalization, whereas the Bayesian framework relies on prior distributions for inducing shrinkage and sparsity. The chapter treats the Bayesian framework in the light of objective and empirical Bayesian viewpoints as the priors in the high-dimensional setting are typically not completely based subjective prior beliefs. An important practical aspect of high-dimensional model selection methods is computational scalability which will also be discussed.}
}
@INPROCEEDINGS{NNStructure,  
    author={Qiu, Xueheng and Zhang, Le and Ren, Ye and Suganthan, P. N. and Amaratunga, Gehan},  
    booktitle={2014 IEEE Symposium on Computational Intelligence in Ensemble Learning (CIEL)},   title={Ensemble deep learning for regression and time series forecasting},   
    year={2014},  
    volume={},  
    number={},  
    pages={1-6},  
    doi={10.1109/CIEL.2014.7015739}
}
@INPROCEEDINGS{LeakyReLU,
    author = {Andrew L. Maas and Awni Y. Hannun and Andrew Y. Ng},
    title = {Rectifier nonlinearities improve neural network acoustic models},
    booktitle = {in ICML Workshop on Deep Learning for Audio, Speech and Language Processing},
    year = {2013}
}
@article{LeakyReLu_a,
  author    = {Bing Xu and
               Naiyan Wang and
               Tianqi Chen and
               Mu Li},
  title     = {Empirical Evaluation of Rectified Activations in Convolutional Network},
  journal   = {CoRR},
  volume    = {abs/1505.00853},
  year      = {2015},
  url       = {http://arxiv.org/abs/1505.00853},
  eprinttype = {arXiv},
  eprint    = {1505.00853},
  timestamp = {Mon, 13 Aug 2018 16:48:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/XuWCL15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{SGD_problem,
	address = {Heidelberg},
	title = {Large-{Scale} {Machine} {Learning} with {Stochastic} {Gradient} {Descent}},
	isbn = {978-3-7908-2604-3},
	abstract = {During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.},
	booktitle = {Proceedings of {COMPSTAT}'2010},
	publisher = {Physica-Verlag HD},
	author = {Bottou, Léon},
	editor = {Lechevallier, Yves and Saporta, Gilbert},
	year = {2010},
	pages = {177--186},
}
@article{Adagrad,
  author  = {John Duchi and Elad Hazan and Yoram Singer},
  title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {61},
  pages   = {2121-2159},
  url     = {http://jmlr.org/papers/v12/duchi11a.html}
}
@article{Momentum,
    author = {Polyak, Boris},
    year = {1964},
    month = {12},
    pages = {1-17},
    title = {Some methods of speeding up the convergence of iteration methods},
    volume = {4},
    journal = {Ussr Computational Mathematics and Mathematical Physics},
    doi = {10.1016/0041-5553(64)90137-5}
}
@misc{Adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@book{DLfromscratch,
  author    = "Koki Saito",
  title     = "Deep Learning from Scratch: Theories and Practice of Deep Learning using Python",
  publisher = "O'Reilly Japan",
  year      = "2016",
  URL       = "https://ci.nii.ac.jp/ncid/BB26486055"
}
@article{optimizers,
  author    = {Sebastian Ruder},
  title     = {An overview of gradient descent optimization algorithms},
  journal   = {CoRR},
  volume    = {abs/1609.04747},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.04747},
  eprinttype = {arXiv},
  eprint    = {1609.04747},
  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Ruder16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{RNN_LSTM_picture,
    author = {Feng, Weijiang and Guan, Naiyang and Li, Yuan and Zhang, Xiang and Luo, Zhigang},
    booktitle={2017 International Joint Conference on Neural Networks (IJCNN)}, 
    year = {2017},
    month = {05},
    pages = {681-688},
    title = {Audio visual speech recognition with multimodal recurrent neural networks},
    doi = {10.1109/IJCNN.2017.7965918}
}
@article{RNN_problem,
  author={Bengio, Y. and Simard, P. and Frasconi, P.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Learning long-term dependencies with gradient descent is difficult}, 
  year={1994},
  volume={5},
  number={2},
  pages={157-166},
  doi={10.1109/72.279181}
 }
@article{LSTM_block_picture,
  author    = {Suman Samui and
               Indrajit Chakrabarti and
               Soumya K. Ghosh},
  title     = {Tensor-Train Long Short-Term Memory for Monaural Speech Enhancement},
  journal   = {CoRR},
  volume    = {abs/1812.10095},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.10095},
  eprinttype = {arXiv},
  eprint    = {1812.10095},
  timestamp = {Mon, 17 Aug 2020 16:23:48 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-10095.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{LSTM_GRU_description,
    author = {Yu, Yong and Si, Xiaosheng and Hu, Changhua and Zhang, Jianxun},
    title = "{A Review of Recurrent Neural Networks: LSTM Cells and Network Architectures}",
    journal = {Neural Computation},
    volume = {31},
    number = {7},
    pages = {1235-1270},
    year = {2019},
    month = {07},
    abstract = "{Recurrent neural networks (RNNs) have been widely adopted in research areas concerned with sequential data, such as text, audio, and video. However, RNNs consisting of sigma cells or tanh cells are unable to learn the relevant information of input data when the input gap is large. By introducing gate functions into the cell structure, the long short-term memory (LSTM) could handle the problem of long-term dependencies well. Since its introduction, almost all the exciting results based on RNNs have been achieved by the LSTM. The LSTM has become the focus of deep learning. We review the LSTM cell and its variants to explore the learning capacity of the LSTM cell. Furthermore, the LSTM networks are divided into two broad categories: LSTM-dominated networks and integrated LSTM networks. In addition, their various applications are discussed. Finally, future research directions are presented for LSTM networks.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01199},
    url = {https://doi.org/10.1162/neco\_a\_01199},
    eprint = {https://direct.mit.edu/neco/article-pdf/31/7/1235/1053200/neco\_a\_01199.pdf},
}
@article{RNN_longtermdependency_issue,
  author    = {Andrej Karpathy and
               Justin Johnson and
               Li Fei{-}Fei},
  title     = {Visualizing and Understanding Recurrent Networks},
  journal   = {CoRR},
  volume    = {abs/1506.02078},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.02078},
  eprinttype = {arXiv},
  eprint    = {1506.02078},
  timestamp = {Wed, 15 Sep 2021 14:13:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KarpathyJL15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{LSTM_forget_gate,
  author={Gers, F.A. and Schmidhuber, J.},
  booktitle={Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium}, 
  title={Recurrent nets that time and count}, 
  year={2000},
  volume={3},
  number={},
  pages={189-194 vol.3},
  doi={10.1109/IJCNN.2000.861302}
 }
 @article{LSTM_GRU_comparison,
  author    = {Roberto Cahuantzi and
               Xinye Chen and
               Stefan G{\"{u}}ttel},
  title     = {A comparison of {LSTM} and {GRU} networks for learning symbolic sequences},
  journal   = {CoRR},
  volume    = {abs/2107.02248},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.02248},
  eprinttype = {arXiv},
  eprint    = {2107.02248},
  timestamp = {Wed, 07 Jul 2021 15:23:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-02248.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}